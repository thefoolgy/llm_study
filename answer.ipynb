{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import defaultdict\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "PAT = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "def read_text(input_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_by_special(text, special_tokens, drop_special=True):\n",
    "    if not special_tokens:\n",
    "        return [text]\n",
    "\n",
    "    # Sort by descending length to prioritize longer tokens (e.g., \"<|endoftext|><|endoftext|>\" before \"<|endoftext|>\")\n",
    "    special_tokens = sorted(special_tokens, key=len, reverse=True)\n",
    "\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    if not drop_special: pattern = f\"({pattern})\"\n",
    "\n",
    "    pattern = re.compile(pattern)\n",
    "    chunks = pattern.split(text)\n",
    "    return [c for c in chunks if c]  # remove empty strings\n",
    "\n",
    "def word2bytes(word):\n",
    "    \"Convert word string to tuple of bytes\"\n",
    "    a = list(word.encode('utf-8'))\n",
    "    return tuple(bytes([i]) for i in a)\n",
    "\n",
    "def count_word(text):\n",
    "    \"Split text into word bytes using GPT2 pattern and count word bytes frequency.\"\n",
    "    word_cnt = defaultdict(int)\n",
    "    for m in PAT.finditer(text):\n",
    "        word = m.group(0)\n",
    "        word_bytes = word2bytes(word)\n",
    "        if len(word_bytes)>=2:\n",
    "            word_cnt[word_bytes]+=1\n",
    "    return word_cnt\n",
    "\n",
    "def merge_dicts(dicts):\n",
    "    merged = defaultdict(int)\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            merged[k] += v\n",
    "    return merged\n",
    "\n",
    "def count_pair(word_cnt):\n",
    "    pair_cnt = defaultdict(int)\n",
    "    for word_bytes,cnt in word_cnt.items():\n",
    "        for pair in zip(word_bytes[:-1],word_bytes[1:]):\n",
    "            pair_cnt[pair]+=cnt\n",
    "    return pair_cnt\n",
    "\n",
    "def get_max_pair(pair_cnt):\n",
    "    max_pair, _ = max(pair_cnt.items(), key=lambda x: (x[1], x[0]))  # lexicographic tie-breaker\n",
    "    return max_pair\n",
    "\n",
    "\n",
    "def get_basic_vocab(special_tokens):\n",
    "    vocab={token:bytes([token]) for token in range(256)}\n",
    "\n",
    "    for i,token in enumerate(special_tokens):\n",
    "        token_id = 256+i\n",
    "        vocab[token_id] = token.encode(\"utf-8\")\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def apply_merge(word_bytes,merge):\n",
    "    merged = merge[0]+merge[1]\n",
    "    i = 0\n",
    "    new_word_bytes = []\n",
    "    while i < len(word_bytes):\n",
    "        # Check for match\n",
    "        if i < len(word_bytes) - 1 and word_bytes[i] == merge[0] and word_bytes[i+1] == merge[1]:\n",
    "            new_word_bytes.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_word_bytes.append(word_bytes[i])\n",
    "            i += 1\n",
    "    return tuple(new_word_bytes)\n",
    "\n",
    "def update_cnt(word_cnt,pair_cnt, merge_pair):\n",
    "\n",
    "    new_word_cnt = defaultdict(int)\n",
    "    new_pair_cnt = defaultdict(int, pair_cnt) # copy with defaultdict\n",
    "\n",
    "    for word_bytes,cnt in word_cnt.items():\n",
    "\n",
    "        #----------for word cnt ---------------\n",
    "\n",
    "        old_pairs = list(zip(word_bytes[:-1], word_bytes[1:]))\n",
    "\n",
    "        # Keep the original count if the merge not appear in the key\n",
    "        if merge_pair not in old_pairs:\n",
    "            new_word_cnt[word_bytes]+=cnt\n",
    "            continue\n",
    "\n",
    "        # Use updated key if merge appear\n",
    "        new_word = apply_merge(word_bytes,merge_pair)\n",
    "        new_word_cnt[new_word]+=cnt\n",
    "\n",
    "        #--------for pair cnt ----------------\n",
    "\n",
    "        # Decrease all old pair counts\n",
    "        for pair in old_pairs:\n",
    "            new_pair_cnt[pair]-=cnt\n",
    "            if new_pair_cnt[pair] ==0:\n",
    "                del new_pair_cnt[pair]\n",
    "\n",
    "        # Count new pairs in the new word\n",
    "        new_pairs = list(zip(new_word[:-1], new_word[1:]))\n",
    "        for p in new_pairs:\n",
    "            new_pair_cnt[p] += cnt\n",
    "\n",
    "    return new_word_cnt,new_pair_cnt\n",
    "\n",
    "\n",
    "def train_bpe(input_path,vocab_size,special_tokens):\n",
    "\n",
    "    text = read_text(input_path)\n",
    "    chunks = split_by_special(text,special_tokens)\n",
    "\n",
    "    # Only parallelize if chunk count is big enough\n",
    "    if len(chunks) < 4: word_dicts = list(map(count_word, chunks))\n",
    "    else: word_dicts = process_map(count_word, chunks, chunksize=1)\n",
    "\n",
    "    word_cnt = merge_dicts(word_dicts)\n",
    "    pair_cnt = count_pair(word_cnt)\n",
    "\n",
    "    vocab = get_basic_vocab(special_tokens)\n",
    "    base_vocab_size = len(vocab)\n",
    "    n_merges=vocab_size-base_vocab_size\n",
    "\n",
    "    merges = []\n",
    "    for i in range(n_merges):\n",
    "        max_pair = get_max_pair(pair_cnt)\n",
    "        vocab[base_vocab_size+i] = max_pair[0]+max_pair[1]\n",
    "        merges.append(max_pair)\n",
    "        word_cnt, pair_cnt = update_cnt(word_cnt,pair_cnt,max_pair)\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/Users/thefoolgy/Desktop/assignment1-basics-main/data/small_test.txt'\n",
    "special_tokens=[\"<|endoftext|>\"]\n",
    "vocab_size = 262\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/Users/thefoolgy/Desktop/assignment1-basics-main/data/small_test.txt'\n",
    "special_tokens=[\"<|endoftext|>\"]\n",
    "vocab, merges = train_bpe(input_path, 262, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'), (b'e', b'st'), (b'o', b'w'), (b'l', b'ow'), (b'w', b'est')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl-fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
